# -*- coding: utf-8 -*-
"""Copy of DRL4VRP.ipynb（用于修改）

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-iPTKQPQ9ePaN_pxE4pRkK6JgSOBxT7r

#DRL4VRP
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""##modle.py
这段代码定义了一个用于解决旅行商问题（TSP）和车辆路径问题（VRP）的深度强化学习模型。它包括编码器、注意力机制和指针网络，用于从给定的城市坐标中学习构建最短路径的策略。模型使用PyTorch框架构建，支持在GPU上运行以加速训练过程。
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.nn.functional as F
import copy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# %matplotlib inline

# device = torch.device('cpu')


class Encoder(nn.Module):
    """Encodes the static & dynamic states using 1d Convolution.
    这是一维卷积（因为地点的坐标和顺序无关，所以用一维卷积当成embed工具，替代RNN编码器来用"""

    def __init__(self, input_size, hidden_size):
        super(Encoder, self).__init__()
        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=1)

    def forward(self, input):
        output = self.conv(input)
        return output  # (batch, hidden_size, seq_len)


class Attention(nn.Module):
    """Calculates attention over the input nodes given the current state."""

    def __init__(self, hidden_size):
        super(Attention, self).__init__()

        # W processes features from static decoder elements
        self.W = nn.Parameter(torch.zeros((1, hidden_size, 3 * hidden_size),  # (1,H,3H)×(B,3H,L)=(B,H,L)
                                          device=device, requires_grad=True))

        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),
                                          device=device, requires_grad=True))  # (1,1,H)×tanh(B,H,L)=(B,1,L)=注意力a

    def forward(self, static_hidden, dynamic_hidden, decoder_hidden):
        batch_size, hidden_size, _ = static_hidden.size()  # B H L

        hidden = decoder_hidden.unsqueeze(2).expand_as(static_hidden)  # 这是小车当前位置的静态信息，本是1BH，扩展成BHL
        hidden = torch.cat((static_hidden, dynamic_hidden, hidden), 1)  # 拼接，B,3H,L

        # Broadcast some dimensions so we can do batch-matrix-multiply
        # 为了能够和batch进行相乘，进行expand操作。expand的参数是目标size，是以复制填充的方式实现的。
        v = self.v.expand(batch_size, 1, hidden_size)  # B,1,H
        W = self.W.expand(batch_size, hidden_size, -1)  # B,H,L

        attns = torch.bmm(v, torch.tanh(torch.bmm(W, hidden)))
        attns = F.softmax(attns, dim=2)  # (batch, seq_len)=(B,L)=注意力a
        return attns


class Pointer(nn.Module):
    """Calculates the next state given the previous state and input embeddings.
    使用GRU部分+指针，根据给定的上一个状态和input embed（当前位置静态信息的embed），
    返回所有节点作为下一个点的概率的分布，以及GRU当前隐状态ht"""

    def __init__(self, hidden_size, num_layers=1, dropout=0.2):
        super(Pointer, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # Used to calculate probability of selecting next state
        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),
                                          device=device, requires_grad=True))

        self.W = nn.Parameter(torch.zeros((1, hidden_size, 2 * hidden_size),
                                          device=device, requires_grad=True))

        # Used to compute a representation of the current decoder output
        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,
                          batch_first=True,
                          dropout=dropout if num_layers > 1 else 0)
        self.encoder_attn = Attention(hidden_size)  # 是注意力块。

        self.drop_rnn = nn.Dropout(p=dropout)
        self.drop_hh = nn.Dropout(p=dropout)

    def forward(self, static_hidden, dynamic_hidden, decoder_hidden, last_hh):
        rnn_out, last_hh = self.gru(decoder_hidden.transpose(2, 1), last_hh)  # 输出RNN output 和当前隐状态last_hh。
        rnn_out = rnn_out.squeeze(1)

        # Always apply dropout on the RNN output 对RNN输出进行dropout……
        rnn_out = self.drop_rnn(rnn_out)
        if self.num_layers == 1:  # 上面init函数里面规定，如果>1就自动drop out，如果=1则需要在这里手动drop out
            # If > 1 layer dropout is already applied
            last_hh = self.drop_hh(last_hh)

        # Given a summary of the output, find an  input context
        enc_attn = self.encoder_attn(static_hidden, dynamic_hidden, rnn_out)  # 在这里计算获得注意力a，B 1 L
        context = enc_attn.bmm(static_hidden.permute(0, 2, 1))  # 只和静态信息相乘！B 1 L × B L H  = B 1 H

        # Calculate the next output using Batch-matrix-multiply ops
        context = context.transpose(1, 2).expand_as(static_hidden)  # 转置成B H 1之后扩展成 B H L
        energy = torch.cat((static_hidden, context), dim=1)  # 和静态信息拼接在一起，(B, num_feats, seq_len) B H L

        v = self.v.expand(static_hidden.size(0), -1, -1)  # 第一维度扩展batch
        W = self.W.expand(static_hidden.size(0), -1, -1)  # 第一维度扩展batch

        probs = torch.bmm(v, torch.tanh(torch.bmm(W, energy))).squeeze(1)  # 得到还没有mask的概率，大小B L

        return probs, last_hh  # 返回每一个点被选取为下一个点的概率，以及隐状态


class DRL4TSP(nn.Module):
    """Defines the main Encoder, Decoder, and Pointer combinatorial models.

    Parameters:
    ----------
    static_size: int
        Defines how many features are in the static elements of the model
        (e.g. 2 for (x, y) coordinates)
    dynamic_size: int > 1
        Defines how many features are in the dynamic elements of the model
        (e.g. 2 for the VRP which has (load, demand) attributes. The TSP doesn't
        have dynamic elements, but to ensure compatility with other optimization
        problems, assume we just pass in a vector of zeros.
    hidden_size: int
        Defines the number of units in the hidden layer for all static, dynamic,
        and decoder output units.
    update_fn: function or None
        If provided, this method is used to calculate how the input dynamic
        elements are updated, and is called after each 'point' to the input element.
    mask_fn: function or None
        Allows us to specify which elements of the input sequence are allowed to
        be selected. This is useful for speeding up training of the networks,
        by providing a sort of 'rules' guidelines to the algorithm. If no mask
        is provided, we terminate the search after a fixed number of iterations
        to avoid tours that stretch forever
    num_layers: int
        Specifies the number of hidden layers to use in the decoder RNN
    dropout: float
        Defines the dropout rate for the decoder
    """

    def __init__(self, static_size, dynamic_size, hidden_size, car_load,
                 update_fn=None, mask_fn=None, node_distance_fn=None, num_layers=1,
                 dropout=0.):  ##########################################################
        super(DRL4TSP, self).__init__()

        if dynamic_size < 1:
            raise ValueError(':param dynamic_size: must be > 0, even if the '
                             'problem has no dynamic elements')

        self.update_fn = update_fn  # 动态信息的更新函数
        self.mask_fn = mask_fn  # mask函数
        self.node_distance_fn = node_distance_fn
        # Define the encoder & decoder models
        self.static_encoder = Encoder(static_size, hidden_size)  # 静态信息embed编码器
        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)  # 动态信息embed编码器
        self.decoder = Encoder(static_size, hidden_size)  # 当前位置信息编码器（为什么叫做decoder
        self.pointer = Pointer(hidden_size, num_layers, dropout)  # 指针网络（含GRU）
        self.car_load = car_load

        for p in self.parameters():
            if len(p.shape) > 1:
                nn.init.xavier_uniform_(p)


        # Used as a proxy initial state in the decoder when not specified 这是小车的初始位置。

        # self.x0 = torch.zeros((1, static_size, 1), requires_grad=True, device=device)

    def forward(self, static, dynamic, decoder_input=None,
                last_hh=None):  ##########################################################
        # todo 删掉 init depot
        """
        Parameters
        ----------
        static: Array of size (batch_size, feats, num_cities)
            Defines the elements to consider as static. For the TSP, this could be
            things like the (x, y) coordinates, which won't change
        dynamic: Array of size (batch_size, feats, num_cities)
            Defines the elements to consider as static. For the VRP, this can be
            things like the (load, demand) of each city. If there are no dynamic
            elements, this can be set to None
        decoder_input: Array of size (batch_size, num_feats)
            Defines the outputs for the decoder. Currently, we just use the
            static elements (e.g. (x, y) coordinates), but this can technically
            be other things as well.
        last_hh: Array of size (batch_size, num_hidden)
            Defines the last hidden state for the RNN
        """
        # 这里已经是坐标的形式了！！！
        batch_size, input_size, sequence_size = static.size()
        distance = self.node_distance_fn(static)  # (B,num_node+1,num_node) # 有一个+1是因为最后一个
        # todo 在多仓问题里面，初始位置需要指定并且不重复。或者在本类的forward第一次调用的时候，就指定decode_input
        if decoder_input is None:
            print(
                "DRL4TSP decoder input is None===================================================!\nself.x0=\n")

        # 获取仓库数量===================================
        num_car = len(decoder_input[0][0])
        num_nodes = static.size(2)
        decoder_input = decoder_input[:, :, 0].unsqueeze(2)

        # Always use a mask - if no function is provided, we don't update it
        mask = torch.ones(batch_size, sequence_size, device=device)  # todo 等等1是mask掉吗

        car_id = 0
        # car_load=[torch.ones(batch_size) for i in range(num_car)] #?????????应该要改成batch size大小的元素吧
        car_load = [torch.full((batch_size,), self.car_load) for _ in range(num_car)]
        # todo =====================================

        # Structures for holding the output sequences
        # tour_idx, tour_logp = [], [] # 最终的访问序列。
        tour_logp = []  # tour idx列表在下面直接用初始仓库序列代替
        # tour idx 的大小是num_car,tour length,B,

        # 使用arange生成初始depot，然后直接调整形状
        initial_depot = torch.arange(num_car).view(num_car, 1, 1)
        # 使用expand复制到所需的批处理大小
        initial_depot = initial_depot.expand(num_car, 1, batch_size)
        initial_depot = initial_depot.tolist()
        # 给每个无人机tour里记录初始仓库。
        tour_idx = [[torch.tensor(initial_depot[i][0]).unsqueeze(1).to(device)] for i in
                    range(num_car)]  # num_car, 1, batch_size
        # fixme=========================================

        # ptr=torch.tensor(tour_idx[0][-1])  #ptr 大小B 1。ptr更新！！！更新为下一个无人机的当前位置。
        ptr = tour_idx[0][-1].clone().detach()  # 当前第0辆无人机的位置。从第一辆无人机开始，取最后一个（其实只有一个元素）所在下标。（维度是batchsiz吗？？）

        max_steps = sequence_size if self.mask_fn is None else 2000  # 如果设置mask函数，为了避免死循环，这是最大步数。
        # distance = self.node_distance(static)
        # Static elements only need to be processed once, and can be used across
        # all 'pointing' iterations. When / if the dynamic elements change,
        # their representations will need to get calculated again.
        static_hidden = self.static_encoder(static)
        dynamic_hidden = self.dynamic_encoder(dynamic)

        for _ in range(max_steps):  # 主循环
            if self.mask_fn is not None:
                # mask = self.mask_fn(dynamic, ptr.data).detach()  # detach是分离出来，但是不需要梯度信息。
                # 根据当前无人机结束新的访问，结合下一台无人机的fixme 注意这个ptr和dynamic需要是新的,因为需要根据下一个无人机的当前位置，判断下一个点不能去哪。
                # mask = self.mask_fn(dynamic, distance, next_car_ptr.data).detach()
                mask = self.mask_fn(dynamic, distance, ptr.data, car_id).detach()

            if not mask.byte().any():  # 如果全mask掉了就退出
                break

            # ... but compute a hidden rep for each element added to sequence
            decoder_hidden = self.decoder(decoder_input)  # 编码当前位置xy静态信息

            # 指针网络。里面包含GRU
            probs, last_hh = self.pointer(static_hidden, dynamic_hidden, decoder_hidden,
                                          last_hh)  # 得到下一个点的（未mask）概率分布和隐状态。
            probs = F.softmax(probs + mask.log(), dim=1)  # mask操作+softmax

            # When training, sample the next step according to its probability.
            # During testing, we can take the greedy approach and choose highest
            if self.training:
                try:
                    m = torch.distributions.Categorical(probs)
                except:
                    print("eeeeee????????")
                    print(dynamic)
                # Sometimes an issue with Categorical & sampling on GPU; See:
                # https://github.com/pemami4911/neural-combinatorial-rl-pytorch/issues/5
                ptr = m.sample()  # 根据上面的概率分布，采样一个点。大小B

                while not torch.gather(mask, 1, ptr.data.unsqueeze(1)).byte().all():
                    ptr = m.sample()

                logp = m.log_prob(ptr)
            else:
                prob, ptr = torch.max(probs, 1)  # Greedy
                logp = prob.log()  # B,1

            # After visiting a node update the dynamic representation 选择好了下一个访问的点，访问，更新动态信息。
            if self.update_fn is not None:
                # dynamic = self.update_fn(dynamic, ptr.data)
                # 获取最后一个访问的点
                # last_visited = tour_idx[-1]
                last_visited = tour_idx[car_id][-1]
                # 更新动态信息，传递最后一个访问的点
                dynamic = self.update_fn(dynamic, distance, ptr.data,
                                         last_visited)  # B 2 L 这里还是【当前小车】的load和当前小车的新一步ptr，更新地图里的load和demand

                # fixme 新增=========================dynamic中的旧load储存，更新新load。所以此后dynamic都是当前无人机访问下一个点后的新环境
                car_load[car_id] = dynamic[:, 0, 0].clone()  # 随便取第一个仓库的load就好了，存到car_load数组里面。
                dynamic[:, 0] = car_load[(car_id + 1) % num_car].unsqueeze(1).expand(-1,
                                                                                     num_nodes)  # 替换dynamic里的load！把load换成下一个无人机的load，但demand继承。
                # fixme =========================

                dynamic_hidden = self.dynamic_encoder(dynamic)  # 得到当前无人机新访问一个点之后的动态环境的hidden

                # Since we compute the VRP in minibatches, some tours may have
                # number of stops. We force the vehicles to remain at the depot
                # in these cases, and logp := 0 （意思应该是batch里面有些情况下，已经遍历完了，就让车强制留在仓库）
                is_done = dynamic[:, 1].sum(1).eq(0).float()  # 如果所有点的需求加和=0，就说明done
                logp = logp * (1. - is_done)  # 如果完成了，logp 也是0, 梯度就不会更新了。

            tour_logp.append(logp.unsqueeze(1))  # 每个时间t都要储存：因为为了计算整条路径出现的概率，所以是logp.sum(). T B 1
            # tour_idx.append(ptr.data.unsqueeze(1))  # T B 1 # 增加tour idx索引
            tour_idx[car_id].append(ptr.data.unsqueeze(1))  # T B 1 把当前无人机的新访问的点保存起来。
            # ptr=torch.tensor(tour_idx[(car_id+1)%num_car][-1])  #ptr 大小B 1。ptr更新！！！更新为下一个无人机的当前位置。
            next_car_ptr = tour_idx[(car_id + 1) % num_car][-1].clone().detach()  # 取出下一台无人机所在的点。

            # 它的ptr是新的！！(改名为next_car_ptr
            # decoder_input = torch.gather(static, 2, ptr.view(-1, 1, 1).expand(-1, input_size, 1).to('cuda')).detach()  # 更新当前位置信息。
            decoder_input = torch.gather(static, 2, next_car_ptr.view(-1, 1, 1).expand(-1, input_size, 1).to(
                dynamic.device)).detach()  # 更新当前位置信息。

            # 车辆序号
            car_id = (car_id + 1) % num_car
            ptr = next_car_ptr
        else:
            print(f"达到最大迭代次数{max_steps}退出")

        # tour_idx = torch.cat(tour_idx, dim=1)  # (batch_size, seq_len)

        tour_idx = [torch.cat(tour_idx[i], dim=1) for i in range(num_car)]  # 包含了每一辆无人机的轨迹
        tour_logp = torch.cat(tour_logp, dim=1)  # (batch_size, seq_len)
        return tour_idx, tour_logp


"""##vrp.py
VehicleRoutingDataset类在初始化时生成一系列随机的VRP实例，包括城市的位置、每个城市的需求量、车辆的最大载重量等。此外，它提供了__getitem__方法来获取单个VRP实例，以及update_mask和update_dynamic方法来更新在解决问题过程中的动态状态，如车辆的当前载重量和城市的剩余需求量。

reward函数用于计算给定路径的总行驶距离，作为优化目标的一部分。

最后，render函数用于将找到的解决方案可视化，通过绘制车辆的行驶路径来展示如何满足所有城市的需求，同时尽可能减少行驶距离。代码中还包含了一个注释掉的render函数版本，该版本使用了matplotlib的动画功能来动态展示路径的构建过程，但默认情况下是不启用的。
"""

"""定义车辆路径问题 (VRP) 的主要任务。

车辆路径问题由以下特征定义：
    1. 每个城市有一个在 [1, 9] 范围内的需求量，必须由车辆服务
    2. 每辆车有一定的容量（取决于问题），必须访问所有城市
    3. 当车辆载重为 0 时，必须返回仓库进行补给
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset
from torch.autograd import Variable
import matplotlib

# matplotlib.use('Agg')  # 防止尝试使用图形界面，允许在没有图形界面的环境中运行 todo ……如果使用谷歌请取消注释
import matplotlib.pyplot as plt


class VehicleRoutingDataset(Dataset):
    def __init__(self, num_samples, num_city, max_load=10, car_load=0, max_demand=1, seed=None,
                 num_depots=-1):  # 增加了 num_depots参数 ###
        super(VehicleRoutingDataset, self).__init__()

        # if max_load < max_demand:
        #     print("max load:", max_load)
        #     print("max demand:", max_demand)
        #     raise ValueError(':param max_load: 必须大于 max_demand')

        if seed is None:
            seed = np.random.randint(1234567890)
        np.random.seed(seed)
        torch.manual_seed(seed)
        '''
            首先carload肯定是会和地图大小有关系。如果绝对值保持1，那么变小的就是city demand……会很奇怪
            所以就不能保持car load=1. 如此一来city demand也不用归一化。
            【注意car load需要大于>demand+地图最远来回路程】
        '''

        self.num_samples = num_samples
        self.num_depots = num_depots  # 保存仓库数量
        self.max_load = max_load  # 原本max load是未归一化的，carload是归一化的1（但是又被我强行改成30了……）
        self.max_demand = max_demand

        # 修改位置生成逻辑以支持多仓库地图
        ##############################
        # locations = torch.rand((num_samples, 2, input_size + 1))
        self.static = torch.rand((num_samples, 2, num_city + self.num_depots))  # 需要生成飞机数量+city数量个节点（前面的是飞机）
        self.car_load = car_load
        ##############################

        # 所有状态都将广播司机当前的载重量
        # 注意，我们只在 [0, 1] 范围内使用载重量，以防止大数字输入神经网络

        ##############################
        # dynamic_shape = (num_samples, 1, input_size + 1)
        dynamic_shape = (num_samples, 1, num_city + self.num_depots)
        ##############################
        # loads = torch.full(dynamic_shape, 5.)
        loads = torch.full(dynamic_shape, car_load)

        # 所有状态都有自己的固有需求量，在 [1, max_demand) 范围内，
        # 然后根据最大载重量进行缩放。例如，如果 load=10 且 max_demand=30，
        # 需求量将缩放到 (0, 3) 范围内
        demands = torch.randint(1, max_demand + 1, dynamic_shape)  # todo 是否要把这里的demand改成相同的？？？？？？
        demands = demands / float(max_load)  # 取消归一化。

        # 设置仓库的需求量为 0
        ##############################
        for depot_idx in range(self.num_depots):
            demands[:, 0, depot_idx] = 0  # 所有仓库的需求量设置为 0
        ##############################

        # 注意这里我们不再需要将 demands 和 loads 分开处理，因为我们已经在 demands 中为仓库预留了位置
        # 因此我们可以直接将 demands 和 loads 合并为一个 dynamic tensor

        ##############################
        # self.dynamic = torch.cat((loads, demands), dim=2)
        self.dynamic = torch.tensor(np.concatenate((loads, demands), axis=1))
        ##############################

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        # 返回 (静态信息, 动态信息, 起始位置) # 现在新增加了返回仓库的下标！！！
        # 生成一个介于0和self.num_depots-1之间的随机索引
        # start_idx = torch.randint(0, self.num_depots, (1,)).item()
        start_idx = torch.arange(0, self.num_depots)  # 0到self.num_depots-1顺序序列，（顺序分配仓库）#????什么意思啊
        return (self.static[idx], self.dynamic[idx], self.static[idx, :, :self.num_depots])  # ????什么意思啊
        # return (self.static[idx], self.dynamic[idx], self.static[idx, :, start_idx:start_idx+1]) # test
        # return (self.static[idx], self.dynamic[idx], self.static[idx, :, 0:1]) # 最后一个变量是仓库。需要改

    def node_distance(self, static):
        '''
        这个版本是计算【点a-点b-点b最近的仓库】的距离的函数。是”可以访问他人仓库“的功能时使用的
        static的维度:应该是B，2，num_depots+num_city
        '''
        depot_positions = static[:, :, :self.num_depots]  # 维度应该是B，2，num_depots
        city_positions = static[:, :, self.num_depots:]  # 维度应该是B，2，num_city
        depot_positions_expanded = depot_positions.unsqueeze(2).expand(-1, -1, city_positions.size(2),
                                                                       -1)  # B,2,num_city, num_depots
        distances_to_depot = torch.sqrt(
            torch.sum((city_positions.unsqueeze(3) - depot_positions_expanded) ** 2, dim=1))  # B,num_city,num_depots
        # 取每行最小值作为每个节点到最近仓库的距离
        min_distances, _ = torch.min(distances_to_depot, dim=2)
        # min_distances = torch.cat((torch.zeros(min_distances.size(0), self.num_depots).to('cuda'), min_distances), dim=1)
        min_distances = torch.cat(
            (torch.zeros(min_distances.size(0), self.num_depots).to(static.device), min_distances), dim=1)
        # 计算所有节点之间的距离
        distances_between_node = torch.sqrt(
            torch.sum((static.unsqueeze(2) - static.unsqueeze(3)) ** 2, dim=1))  # 计算欧式距离
        distances = torch.cat((distances_between_node, min_distances.unsqueeze(1)), dim=1)
        # distances大小为 (betch_size,seq_len+1,seq_len)加上了最近仓库张量
        return distances

    def node_distance2(self, static):
        """
        和楼上不一样！这个函数需要适用于点到仓库的距离/fixme 记得在切换使用的函数。
        之前的版本return 的维度是B，num_node+1,num_node.属于是强行在第二个维度的最后面上添加了一个大小为num_node的矩阵……
        所以返回值的[b][-1][i]表示在第b个batch里面，第i个node离最近的仓库的距离。（如果第i是仓库则距离=0）
        """

        # # 既然要改成只能回到自己的机场，这个之后需要修改。
        # depot_positions = static[:, :, :self.num_depots]# 维度应该是B，2，num_depots
        # city_positions = static[:, :, self.num_depots:]# 维度应该是B，2，num_city
        # depot_positions_expanded = depot_positions.unsqueeze(2).expand(-1, -1, city_positions.size(2), -1)# B,2,num_city, num_depots
        # # 计算每个节点到最近仓库的距离
        # distances_to_depot = torch.sqrt(torch.sum((city_positions.unsqueeze(3) - depot_positions_expanded) ** 2, dim=1))# B,num_city,num_depots
        # # 取每行最小值，作为每个city节点到最近仓库的距离 fixme 这个应该不用再取最小值了，这行要删掉。
        # min_distances, _ = torch.min(distances_to_depot, dim=2) # (B,num_city)
        # min_distances = torch.cat( # 这一行本意是在前面加上“每个仓库距离最近的仓库（自己）距离为0”的意思） fixme
        #     (torch.zeros(min_distances.size(0), self.num_depots).to(static.device), min_distances), dim=1)
        # todo 要在这里写 "每一个city到每一个仓库的距离“矩阵（可以参考上面）【算了别写了】

        # 计算所有节点之间的距离 fixme ？？似乎不用删除。看看mask和update是如何使用的。
        distances_between_node = torch.sqrt(
            torch.sum((static.unsqueeze(2) - static.unsqueeze(3)) ** 2, dim=1))  # 计算欧式距离
        # distances = torch.cat((distances_between_node, min_distances.unsqueeze(1)), dim=1)

        return distances_between_node  # distances # distances大小为 (betch_size,seq_len+1,seq_len).加上了最近仓库张量

    def update_mask(self, dynamic, distance, chosen_idx=None):
        """更新用于隐藏非有效状态的掩码。

        参数
        ----------
        dynamic: torch.autograd.Variable 的大小为 (1, num_feats, seq_len)
        chosen_idx:[非常重要] 是当前的无人机的坐标。需要根据当前坐标，mask下一个可能的点。
        """

        # 将浮点数转换为整数进行计算
        loads = dynamic.data[:, 0]  # (batch_size, seq_len)
        demands = dynamic.data[:, 1]  # (batch_size, seq_len)
        nodedistance = distance + distance[:, -1, :].unsqueeze(1)  # 节点之间的距离加上与最近仓库的距离。
        nodedistance = nodedistance[:, :-1, :]  # fixme 着什么距离啊……要改。

        # 计算 chosen_idx 到仓库点的距离
        depot_distances = distance[torch.arange(distance.size(0)), chosen_idx.squeeze(1), :self.num_depots]
        # 将 chosen_distance 的前 self.num_depots 个元素替换为 chosen_idx 到仓库点的距离。 todo 检查。
        chosen_distance = nodedistance[torch.arange(nodedistance.size(0)), chosen_idx.squeeze(1)]
        chosen_distance[:, :self.num_depots] = depot_distances

        # 如果没有剩余的正需求量，我们可以结束行程。
        if demands.eq(0).all():  # 即所有batch的里面，地图里面每一个点都没有需求了：
            return demands * 0.

        # todo 这个 demand ne 0 会筛选出：所有有需求的city+所有空的仓库节点
        #  第二项三项筛选出并且loads-chosen_distance需要大于0的city【虽然这一条会涉及到仓库，但是先假设后续会单独对仓库进行处理，这里怎么样都不管】
        new_mask = demands.ne(0) * demands.lt(loads - chosen_distance) * (loads - chosen_distance > 0)

        # 我们应该避免连续两次前往仓库
        # repeat_home = chosen_idx.ne(0)
        # if repeat_home.any(): # 不在仓库的可以回去。
        #     new_mask[repeat_home.nonzero(), 0] = 1.
        # if (~repeat_home).any(): # 在仓库的不能继续访问仓库
        #     new_mask[(~repeat_home).nonzero(), 0] = 0.
        ############################# 这段是避免连续两次前往仓库（"如果在仓库，下一个就不可以访问任何仓库"）
        for depot_idx in range(self.num_depots):
            repeat_home = chosen_idx == depot_idx
            if repeat_home.any():
                new_mask[repeat_home.squeeze().nonzero(), :self.num_depots] = 0
        ##############################

        # ...除非我们在等待小批量中的其他样本完成
        # has_no_load = loads[:, 0].eq(0).float() # 仓库load=0 说明无人机归位。
        # has_no_demand = demands[:, 1:].sum(1).eq(0).float() # 这里的1要改/所有city都没有demand
        has_no_demand = demands[:, self.num_depots:].sum(1).eq(0).float()  # 所有city都没有demand，转1和0 【避免本无人机在仓库但是其他无人机还没回去）

        # combined = (has_no_load + has_no_demand).gt(0) # combine zero：该样本有的 city有demand 并且 车load不等于0
        combined = has_no_demand.gt(0)  # combined应该是B 1 吧？
        if combined.any():  # 如果该batch里面存在city没有demand（也就是说有的batch结束了，需要让无人机允许留在原地）
            # 首先，我们将所有节点的掩码设置为0，防止访问
            # new_mask[combined.nonzero(), :] = 0.
            # 对于每个样本，如果它已经在仓库中，我们只允许它访问当前所在的仓库
            # for sample_idx in combined.nonzero().squeeze():
            for sample_idx in combined.nonzero():  # 找到全部无需求的batch id
                current_location = chosen_idx[sample_idx]  # 找到当前位置
                if current_location < self.num_depots:  # 如果当前在仓库
                    # 仅允许访问当前所在的仓库 todo 这个应该不用改？
                    # new_mask[combined.nonzero(), :] = 0.
                    new_mask[sample_idx, current_location] = 1.
                # else:
                #     # 如果不在仓库，但没有需求或者载重为0，则允许访问【demand不为0的仓库】
                #     #new_mask[sample_idx, :self.num_depots] = 1.
                #     new_mask[sample_idx * torch.ones_like(demands[sample_idx], dtype=torch.long), demands[sample_idx] != 0] = 1.

        # 判断是否存在某一行的mask全为0#################### todo 这个是打底的补丁，先注释掉看看之后会不会引起异常报错
        all_zero_mask = (new_mask == 0).all(dim=1)
        if all_zero_mask.any():
            # 找到全为0的行的索引
            all_zero_indices = all_zero_mask.nonzero(as_tuple=True)[0]
            # 将这些行中chosen_idx对应位置的mask设为1
            new_mask[all_zero_indices, chosen_idx[all_zero_indices]] = 1
            print("error:存在某一行的mask全为0-------------------------------")

        return new_mask.float()

    def update_mask2(self, dynamic, n2n_distance, current_idx, car_id):
        """和上一个相比是只允许无人机返回自己的仓库。

        dynamic: torch.autograd.Variable 的大小为 (1, num_feats, seq_len)
        chosen_idx:[非常重要] 大小(B,1)是当前的无人机的坐标。需要根据当前坐标，mask下一个可能的点。
        """

        # 将浮点数转换为整数进行计算
        loads = dynamic.data[:, 0]  # (batch_size, seq_len)
        demands = dynamic.data[:, 1]  # (batch_size, seq_len)

        # fixme 关于距离和需求的处理，要改。==========================
        '''
        9.28修改：这里只考虑mask city的逻辑：(就算是影响到仓库也没关系，后续会处理仓库进行覆盖。)
            找到当前的位置，并且取出当前点-所有点的距离+所有点回自己的仓库（已经给了carid）的距离 
            （所以我需要node-node 矩阵就够了。因为可以转换成：D(当前点~所有点) +D(自己仓库~所有点)
        '''
        # n2n_distance 是维度为(B,node_num,node_num)的
        dis_cur2all=n2n_distance[torch.arange(n2n_distance.size(0)),current_idx.squeeze(1)]
        dis_depot2all=n2n_distance[torch.arange(n2n_distance.size(0)),car_id] # 仓库id=当前car id

        dis_cur2node2depot=dis_cur2all+dis_depot2all # D(当前点~所有点) +D(自己仓库~所有点) 总距离

        # nodedistance = distance[:, :-1, :] + distance[:, -1, :].unsqueeze(1) #(B,node_num, node_num)# 节点之间的距离加上与最近仓库的距离。
        #
        # # 计算 chosen_idx 到仓库点的距离
        # depot_distances = distance[torch.arange(distance.size(0)), current_idx.squeeze(1), :self.num_depots]
        # # 将 chosen_distance 的前 self.num_depots 个元素替换为 chosen_idx 到仓库点的距离。 todo 检查。
        # chosen_distance = nodedistance[torch.arange(nodedistance.size(0)), current_idx.squeeze(1)]
        # chosen_distance[:, :self.num_depots] = depot_distances
        ######################## fixme以上要修改。

        # 如果没有剩余的正需求量，我们可以结束行程。
        if demands.eq(0).all():  # 即所有batch的里面，地图里面每一个点都没有需求了：
            return demands * 0.

        # 这个 demand ne 0 会筛选出：所有有需求的city+所有空的仓库节点
        # 第二项三项筛选出并且loads-chosen_distance需要大于0的city【虽然这一条会涉及到仓库，但是先假设后续会单独对仓库进行处理，这里怎么样都不管】
        new_mask = demands.ne(0) * demands.lt(loads - dis_cur2node2depot) * (loads - dis_cur2node2depot > 0)

        ##############################
        # 9.28修改方案：任何时刻兜底把所有仓库mask掉.
        new_mask[:, :self.num_depots] = 0
        # 然后判断让不在仓库的可以回到自己的仓库。……完了不知道是第几号。
        in_city = (current_idx >= self.num_depots)
        new_mask[in_city.squeeze(), car_id] = 1

        # ...除非我们在等待小批量中的其他样本完成
        # has_no_load = loads[:, 0].eq(0).float() # 仓库load=0 说明无人机归位。
        # has_no_demand = demands[:, 1:].sum(1).eq(0).float() # 这里的1要改/所有city都没有demand
        has_no_demand = demands[:, self.num_depots:].sum(1).eq(0).float()  # 所有city都没有demand，转1和0 【避免本无人机在仓库但是其他无人机还没回去）

        # combined = (has_no_load + has_no_demand).gt(0) # combine zero：该样本有的 city有demand 并且 车load不等于0
        combined = has_no_demand.gt(0)  # combined应该是B 1 吧？
        if combined.any():  # 如果该batch里面存在city没有demand（也就是说有的batch结束了，需要让无人机允许留在原地）
            # 首先，我们将所有节点的掩码设置为0，防止访问
            # new_mask[combined.nonzero(), :] = 0.
            # 对于每个样本，如果它已经在仓库中，我们只允许它访问当前所在的仓库
            # for sample_idx in combined.nonzero().squeeze():
            for sample_idx in combined.nonzero():  # 找到全部无需求的batch id
                current_location = current_idx[sample_idx]  # 找到当前位置
                if current_location < self.num_depots:  # 如果当前在仓库
                    # 仅允许访问当前所在的仓库
                    # new_mask[combined.nonzero(), :] = 0.
                    new_mask[sample_idx, current_location] = 1.
                # else:
                #     # 如果不在仓库，但没有需求或者载重为0，则允许访问【demand不为0的仓库】
                #     #new_mask[sample_idx, :self.num_depots] = 1.
                #     new_mask[sample_idx * torch.ones_like(demands[sample_idx], dtype=torch.long), demands[sample_idx] != 0] = 1.

        # 判断是否存在某一行的mask全为0#################### todo 这个是打底的补丁，先注释掉看看之后会不会引起异常报错
        all_zero_mask = (new_mask == 0).all(dim=1)
        if all_zero_mask.any():
            # 找到全为0的行的索引
            # all_zero_indices = all_zero_mask.nonzero(as_tuple=True)[0]
            # 将这些行中chosen_idx对应位置的mask设为1
            # new_mask[all_zero_indices, chosen_idx[all_zero_indices]] = 1
            print("error:存在某一行的mask全为0-------------------------------")

        return new_mask.float()

    def update_dynamic(self, dynamic, distance, next_idx, current_idx):  # 加了参数：访问的前一个点。
        """
        用于更新当前地图的dynamic的函数。啊要用到distance是因为dynamic里面的load需要减去距离……todo load-=demand+distance
        """
        # print("update_dynamic:网络预测下一步next_idx 是：\n",next_idx)
        # print("update_dynamic:当前所在位置current_idx 是：\n",current_idx)
        """更新 (load, demand) 的值。"""
        current_idx = current_idx.squeeze()
        # 根据是访问仓库还是城市，以不同方式更新动态元素
        ##############################
        visit = next_idx.ge(self.num_depots)  # 访问的是城市还是仓库
        depot = next_idx.lt(self.num_depots)
        # 如果 chosen_idx 小于 num_depots，则表示访问的是仓库
        ##############################

        # 克隆动态变量，以免破坏图
        all_loads = dynamic[:, 0].clone()
        all_demands = dynamic[:, 1].clone()
        load = torch.gather(all_loads, 1, next_idx.unsqueeze(1))  # 获得batch里每一个样本，下一个节点的load
        demand = torch.gather(all_demands, 1, next_idx.unsqueeze(1))  # 获得batch里每一个样本，下一个节点的demand
        distance_matrix = distance[:, :-1, :]  # 距离矩阵 todo等等为什么是取到-1啊！！！！【似乎是特殊的含义吗……】
        # 在小批量中 - 如果我们选择访问一个城市，尽可能满足其需求量
        if visit.any():
            diff_distances = distance_matrix[
                torch.arange(distance_matrix.size(0)), current_idx, next_idx.squeeze()].unsqueeze(1)
            # 上一次选择的节点与这次选择的节点的差值
            new_load = torch.clamp(load - demand - diff_distances, min=0)
            new_demand = torch.clamp(demand - load, min=0)

            # 将载重量广播到所有节点，但单独更新需求量
            visit_idx = visit.nonzero().squeeze()

            all_loads[visit_idx] = new_load[visit_idx]
            all_demands[visit_idx, next_idx[visit_idx]] = new_demand[visit_idx].view(-1)
            # all_demands[visit_idx, 0] = -1. + new_load[visit_idx].view(-1) # 改了

        # -----------测试把上一个访问节点是仓库的时候，条件扩展为"当前访问节点可以是任何点（即允许连续两次访问仓库）
        # 使用布尔索引来找出上一个访问的是仓库的样本
        # is_depot = last_visited.lt(self.num_depots).to('cuda')
        depot_visited_idx = current_idx.lt(self.num_depots).to(dynamic.device)

        # depot_visited_idx = is_depot # 找出同时访问城市且上一次访问的是仓库的样本（不用了，需要可以连续访问仓库）

        # 1，原始的visit idx是【数组下标】不是城市序号！！ todo 喂真的是下标吗 不是bool吗
        depot_visited_idx = depot_visited_idx.nonzero().squeeze()

        # 使用布尔索引和高效的张量操作来更新all_demands
        # all_demands[depot_visited_idx.to('cuda'), last_visited.to('cuda')[depot_visited_idx]] = -1. #+ new_load[depot_visited_idx].reshape(-1,2)
        all_demands[depot_visited_idx.to(dynamic.device), current_idx.to(dynamic.device)[
            depot_visited_idx]] = -1.  # + new_load[depot_visited_idx].reshape(-1,2)

        # 返回仓库以填充车辆载重量
        ##############################
        # if depot.any():
        #     all_loads[depot.nonzero().squeeze()] = 1.
        #     all_demands[depot.nonzero().squeeze(), 0] = 0.
        if depot.any():
            # all_loads[depot.nonzero().squeeze()] = float(self.max_load)#fixme load值
            all_loads[depot.nonzero().squeeze()] = float(self.car_load)
            depot_indices = next_idx[depot.squeeze()]
            all_demands[depot.squeeze(), depot_indices] = 0.
        ##############################

        new_dynamic = torch.cat((all_loads.unsqueeze(1), all_demands.unsqueeze(1)), 1)
        # return torch.tensor(tensor.data, device=dynamic.device)
        # 避免额外的计算开销和不必要的内存使用
        # print("update_dynamic:当前地图的dynamic是\n",tensor)
        return new_dynamic.clone().detach().to(device=dynamic.device)

    def update_dynamic2tem(self, dynamic, distance, next_idx, current_idx):  # 加了参数：访问的前一个点。
        """
        这是用于更新当前地图的dynamic的函数。啊要用到distance是因为dynamic里面的load需要减去距离……todo load-=demand+distance
        current_idx：如果飞机离开了仓库，则需要更新仓库demand=-1以标记已经为空。
        next_idx：本质上我们认为飞机【已经去了】下一个节点 。所以下一个节点的需求、飞机的load也要根据节点类型而更新。
        """
        # print("update_dynamic:网络预测下一步next_idx 是：\n",next_idx)
        # print("update_dynamic:当前所在位置current_idx 是：\n",current_idx)

        current_idx = current_idx.squeeze()
        # 根据【下一个节点】是访问仓库还是城市，以不同方式更新dynamic
        ##############################
        visit = next_idx.ge(self.num_depots)  # 下一个节点访问的是城市还是仓库
        depot = next_idx.lt(self.num_depots)
        ##############################

        # 克隆动态变量，以免破坏图
        all_loads = dynamic[:, 0].clone()
        all_demands = dynamic[:, 1].clone()
        load = torch.gather(all_loads, 1, next_idx.unsqueeze(1))  # 获得batch里每一个样本，下一个节点的load
        demand = torch.gather(all_demands, 1, next_idx.unsqueeze(1))  # 获得batch里每一个样本，下一个节点的demand
        distance_matrix = distance[:, :-1, :]  # 距离矩阵 todo等等为什么是取到-1啊！！！！【似乎是特殊的含义吗……】
        # 在batch中 - 如果我们下一个点选择访问一个城市：
        if visit.any():
            diff_distances = distance_matrix[  # fixme 这里距离还要修改
                torch.arange(distance_matrix.size(0)), current_idx, next_idx.squeeze()].unsqueeze(1)
            # 上一次选择的节点与这次选择的节点的差值
            new_load = torch.clamp(load - demand - diff_distances, min=0)  # fixme ……这个clamp……
            new_demand = torch.clamp(demand - load, min=0)

            # 将载重量广播到所有节点，但单独更新需求量
            visit_idx = visit.nonzero().squeeze()

            all_loads[visit_idx] = new_load[visit_idx]
            all_demands[visit_idx, next_idx[visit_idx]] = new_demand[visit_idx].view(-1)
            # all_demands[visit_idx, 0] = -1. + new_load[visit_idx].view(-1) # 改了

        # ----------------测试把上一个访问节点是仓库的时候，条件扩展为"当前访问节点可以是任何点（即允许连续两次访问仓库）
        #  TT终于想起来了：dynamic会标记仓库是非空（-1空、0非空），所以update的时候，如果无人机离开仓库（当前点=仓库），就要更新该仓库demand
        #  注意，本质上我们认为飞机【已经去了】下一个节点 next_idx。所以下一个节点的需求、飞机的load也要根据节点类型而更新。
        # 使用布尔索引来找出当前节点的是仓库的样本
        depot_visited = current_idx.lt(self.num_depots).to(dynamic.device)

        # depot_visited_idx是【数组下标】不是城市序号！！
        depot_visited_idx = depot_visited.nonzero().squeeze()

        # 使用布尔索引和高效的张量操作来更新all_demands【意思是：把当前节点是depot的样本，的对应depot的demand更新为-1
        # all_demands[depot_visited_idx.to('cuda'), last_visited.to('cuda')[depot_visited_idx]] = -1. #+ new_load[depot_visited_idx].reshape(-1,2)
        all_demands[depot_visited_idx.to(dynamic.device), current_idx.to(dynamic.device)[
            depot_visited_idx]] = -1.  # + new_load[depot_visited_idx].reshape(-1,2)
        # -------------------------

        # 在batch中 - 如果我们下一个选择访问一个仓库
        # if depot.any():
        #     all_loads[depot.nonzero().squeeze()] = 1.
        #     all_demands[depot.nonzero().squeeze(), 0] = 0.
        if depot.any():
            all_loads[depot.nonzero().squeeze()] = float(self.car_load)  # todo 检查这个car load的意思
            all_demands[depot.squeeze(), next_idx[depot.squeeze()]] = 0.

        new_dynamic = torch.cat((all_loads.unsqueeze(1), all_demands.unsqueeze(1)), 1)
        return new_dynamic.clone().detach().to(device=dynamic.device)  # 避免额外的计算开销和不必要的内存使用


def reward(static, tour_indices, depot_number):  # 这个是旧的reward，,depot_number参数没用
    """
    根据 tour_indices 给出的所有城市/节点之间的欧几里得距离

    参数:
    static: 包含所有城市/节点位置的静态信息张量。
    tour_indices: 由城市/节点索引构成的序列，表示访问的顺序。

    返回:
    旅行总长度: 计算得到的旅行的总欧几里得距离。
    """
    total_len = []
    for tour_indices_item in tour_indices:
        # 将索引转换回旅行路线
        idx = tour_indices_item.unsqueeze(1).expand(-1, static.size(1), -1)
        # print("reward 内的原始idx是：\n",idx)
        tour = torch.gather(static.data, 2, idx).permute(0, 2, 1)

        # 确保我们总是返回到仓库 - 注意额外的 concat
        # 不会增加任何额外的损失，因为连续点之间的欧几里得距离是 0

        # start = static.data[:, :, 0].unsqueeze(1) # 取消了在开头结尾补充仓库。结尾一定有仓库，开头的仓库需要手动添加。！！！！！
        # y = torch.cat((start, tour, start), dim=1)

        # 每个连续点之间的欧几里得距离
        # tour_len = torch.sqrt(torch.sum(torch.pow(y[:, :-1] - y[:, 1:], 2), dim=2))
        tour_len = torch.sqrt(torch.sum(torch.pow(tour[:, :-1] - tour[:, 1:], 2), dim=2))
        total_len.append(tour_len.sum(1))
    # 返回旅行总长度
    # total_len.sum(1)
    # print(total_len)

    # return total_len#tour_len.sum(1)

    # 将列表中的所有元素堆叠成一个新的张量
    total_len_tensor = torch.stack(total_len)
    # print(total_len_tensor)
    # 计算所有旅行的总长度之和
    total_distance = total_len_tensor.sum(0)
    # print("____________reward",total_distance)

    return total_distance


def reward2(static, tour_indices, depot_number):
    """
    根据 tour_indices 给出的所有城市/节点之间的欧几里得距离

    参数:
    static: 包含所有城市/节点位置的静态信息张量。
    tour_indices: 由城市/节点索引构成的序列，表示访问的顺序。

    返回:
    旅行总长度: 计算得到的旅行的总欧几里得距离。
    """
    total_len = []
    for tour_indices_item in tour_indices:  # tour indice 维度：car_num, batch size, tour len, 这里对每一个编号的车枚举
        # 将索引转换回旅行路线
        idx = tour_indices_item.unsqueeze(1).expand(-1, static.size(1), -1)
        tour = torch.gather(static.data, 2, idx).permute(0, 2, 1)  # static: B,2,L, tour：B，L，2

        # 改进思路：1，把返回仓库的路径长度*2
        # 2,直接使用返回仓库的次数，*一个常数值去优化。比如1.（先写这个吧比较简单）

        # 2：tour_indices_item 的维度是B，L

        return_depot_times = (tour_indices_item < depot_number).float()
        return_depot_times = return_depot_times.sum(1)  # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!找时间修改成-1再跑一次！！！！！！

        # 每个连续点之间的欧几里得距离
        tour_len = torch.sqrt(torch.sum(torch.pow(tour[:, :-1] - tour[:, 1:], 2), dim=2))
        total_len.append(tour_len.sum(1))

    # 将列表中的所有元素堆叠成一个新的张量
    total_len_tensor = torch.stack(total_len)
    # 计算所有旅行的总长度之和
    total_distance = total_len_tensor.sum(0)
    return total_distance + return_depot_times


def render(static, tour_indices, num_depots, save_path):
    """绘制找到的解决方案。"""
    # 关闭所有潜在的之前的绘图
    plt.close('all')

    # 确定绘制的子图数量，至少绘制一张图
    num_plots = 2

    # 创建子图
    _, axes = plt.subplots(nrows=num_plots, ncols=num_plots,
                           sharex='col', sharey='row')
    # 如果只有一个子图，确保 axes 是二维的
    if num_plots == 1:
        axes = [[axes]]
    # 将 axes 从二维列表转换为一维列表
    axes = [a for ax in axes for a in ax]
    colors = ['red', 'blue', 'green', 'orange', 'purple', 'yellow', 'cyan', 'magenta', 'brown', 'gray']  # 创建颜色列表
    # 遍历每个子图，绘制路径
    for i, ax in enumerate(axes):
        for j in range(len(tour_indices)):
            # Convert the indices back into a tour
            idx = tour_indices[j][i]
            if len(idx.size()) == 1:
                idx = idx.unsqueeze(0)
            idx = idx.expand(static.size(1), -1)
            print("render function", idx)
            data = torch.gather(static[i].data, 1, idx).cpu().numpy()
            # start = static[i, :, 0].cpu().data.numpy()
            # x = np.hstack((start[0], data[0], start[0]))
            # y = np.hstack((start[1], data[1], start[1]))
            x = np.hstack(data[0])
            y = np.hstack(data[1])
            # Assign each subtour a different colour & label in order traveled
            # idx = np.hstack((0, tour_indices[i].cpu().numpy().flatten(), 0))
            # where = np.where(idx == 0)[0]
            idx = np.hstack(tour_indices[j][i].cpu().numpy().flatten())
            where = np.where(idx < num_depots)[0]
            for k in range(len(where) - 1):
                low = where[k]
                high = where[k + 1]
                ax.plot(x[low: high + 1], y[low: high + 1], zorder=1, color=colors[j % 10], label=j)

            # ax.legend(loc="upper right", fontsize=5, framealpha=0.5)
            # 给每个点加上它的序号################
            for point_idx, (px, py) in enumerate(zip(x, y)):
                ax.text(px, py, str(idx[point_idx]), fontsize=8, ha='right', va='bottom')

        ax.scatter(static[i, 0, num_depots:].cpu(), static[i, 1, num_depots:].cpu(), s=10, c='r', zorder=2)
        ax.scatter(static[i, 0, :num_depots].cpu(), static[i, 1, :num_depots].cpu(), s=50, c='k', marker='*', zorder=3)

        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
    # plt.show()
    plt.tight_layout()
    plt.savefig(save_path, bbox_inches='tight', dpi=500)


"""##trainer.py
主要组件

* StateCritic类：估计给定问题的复杂性。它接受静态和动态输入，并通过一系列的全连接层（Conv1d）来估计问题的复杂性或成本。

* Critic类：一个简化版的StateCritic，用于估计问题的复杂性。它通过全连接层处理输入，输出问题复杂性的估计值。

* train函数：负责训练过程，包括前向传播、计算奖励、计算损失、反向传播和参数更新。它同时训练Actor和Critic网络。

* validate函数：在验证集上评估模型性能，计算平均奖励，并可选地渲染和保存解决方案的图像。

* train_tsp和train_vrp函数：这两个函数分别用于设置和训练TSP和VRP问题的模型。它们加载数据集、初始化模型和优化器，并调用train函数进行训练。

* 命令行参数解析：代码的最后部分解析命令行参数，以便用户可以指定训练的任务类型（TSP或VRP）、节点数、学习率、批大小等参数。
"""

# Commented out IPython magic to ensure Python compatibility.
"""Defines the main trainer model for combinatorial problems

Each task must define the following functions:
* mask_fn: can be None
* update_fn: can be None
* reward_fn: specifies the quality of found solutions
* render_fn: Specifies how to plot found solutions. Can be None
"""

import os
import time
import argparse
import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from ipywidgets import interact, interactive, fixed, interact_manual

# %matplotlib inline
# from model import DRL4TSP, Encoder
# import DRL4TSP, Encoder
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"using device:{device}")


# device = torch.device('cpu')


class StateCritic(nn.Module):
    # 简单的使用model里的Encoder方法以及三次一维卷积得到Critic的结果
    """Estimates the problem complexity.

    This is a basic module that just looks at the log-probabilities predicted by
    the encoder + decoder, and returns an estimate of complexity
    """

    def __init__(self, static_size, dynamic_size, hidden_size):
        super(StateCritic, self).__init__()

        self.static_encoder = Encoder(static_size, hidden_size)
        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)

        # Define the encoder & decoder models
        self.fc1 = nn.Conv1d(hidden_size * 2, 20, kernel_size=1)
        self.fc2 = nn.Conv1d(20, 20, kernel_size=1)
        self.fc3 = nn.Conv1d(20, 1, kernel_size=1)

        for p in self.parameters():
            if len(p.shape) > 1:  # 张量的行数应该大于1，从而满足xavier的使用条件
                nn.init.xavier_uniform_(p)  # xavier预防一些参数过大或过小的情况，再保证方差一样的情况下进行缩放，便于计算

    def forward(self, static, dynamic):

        # Use the probabilities of visiting each
        # 静态和动态数据编码
        static_hidden = self.static_encoder(static)
        dynamic_hidden = self.dynamic_encoder(dynamic)
        # 讲两种编码后第二个维度上拼接，就是按照Conv1d的输出结果个数(hidden_size)进行拼接，得到2*hidden_size
        hidden = torch.cat((static_hidden, dynamic_hidden), 1)

        output = F.relu(self.fc1(hidden))
        output = F.relu(self.fc2(output))
        # 提取特征后，第二维求和，得到结果
        output = self.fc3(output).sum(dim=2)
        return output


def validate(data_loader, actor, reward_fn, render_fn=None, save_dir='.',
             num_plot=5, depot_number=-1):
    """Used to monitor progress on a validation set & optionally plot solution."""
    # 将actor设置为评估模式，确保不会应用随机性或梯度计算
    actor.eval()

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    rewards = []
    for batch_idx, batch in enumerate(data_loader):

        # static, dynamic, x0, init_depot = batch
        static, dynamic, x0 = batch

        static = static.to(device)  # 复制变量到GPU上
        dynamic = dynamic.to(device)
        x0 = x0.to(device) if len(x0) > 0 else None

        with torch.no_grad():  # 把requires_grad设置为False,避免反向传播时自动求导，节约了显存

            ############################
            # init_depot=init_depot.to(device)
            # init_depot=init_depot.unsqueeze(1)
            ############################
            # 关键步骤！！！，把静态数据和动态数据进行actor操作，获得访问节点的索引。
            # tour_indices, _ = actor.forward(static, dynamic,init_depot, x0)
            tour_indices, _ = actor.forward(static, dynamic, x0)

        # 使用vrp奖励函数 reward_fn 计算预测的旅游索引的奖励。取奖励的均值，并使用 item() 提取标量值，添加答rewards列表中
        reward = reward_fn(static, tour_indices, depot_number).mean().item()
        rewards.append(reward)
        # 控制vrp的渲染函数，主要是于作图有关
        if render_fn is not None and batch_idx < num_plot:
            name = 'batch%d_%2.4f.png' % (batch_idx, reward)
            print(f"Picture:{name}")
            path = os.path.join(save_dir, name)
            render_fn(static, tour_indices, x0.size(2), path)
    # 将模型 actor 设置回训练模式
    actor.train()
    # 返回平均奖励
    return np.mean(rewards)


def train(actor, critic, task, num_nodes, train_data, valid_data, reward_fn,
          render_fn, batch_size, actor_lr, critic_lr, max_grad_norm,
          depot_num, **kwargs):
    # 搭建主要的AC网络，进行全部的训练
    """Constructs the main actor & critic networks, and performs all training."""
    # 时间，保存路径获取
    google_drive_path = '/content/drive/MyDrive/'
    now = '%s' % datetime.datetime.now().time()
    now = now.replace(':', '_')
    save_dir = os.path.join(task, '%d' % num_nodes, now)
    # 判断是否有checkpoint
    checkpoint_dir = os.path.join(save_dir, 'checkpoints')
    checkpoint_dir = os.path.join(google_drive_path, checkpoint_dir)  # ！！！！！！！！！把路径设置成了谷歌硬盘的路径。
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    # 定义AC优化器
    actor_optim = optim.Adam(actor.parameters(), lr=actor_lr)
    critic_optim = optim.Adam(critic.parameters(), lr=critic_lr)

    train_loader = DataLoader(train_data, batch_size, True, num_workers=0)  # 读取训练数据，一次读取batch_size个，无序
    valid_loader = DataLoader(valid_data, batch_size, False, num_workers=0)  # 读取测试数据，一次读取batch_size个，有序

    best_params = None
    best_reward = np.inf  # 正无穷大
    all_epoch_loss, all_epoch_reward = [], []
    for epoch in range(5):  # 执行20轮训练 fixme!!!!!!!!

        actor.train()
        critic.train()

        times, losses, rewards, critic_rewards = [], [], [], []

        epoch_start = time.time()
        start = epoch_start

        for batch_idx, batch in enumerate(train_loader):

            # static, dynamic, x0 ,init_depot= batch
            static, dynamic, x0 = batch

            static = static.to(device)
            dynamic = dynamic.to(device)
            x0 = x0.to(device) if len(x0) > 0 else None

            ############################
            # init_depot=init_depot.to(device)
            # init_depot=init_depot.unsqueeze(1)
            ############################
            # Full forward pass through the dataset(使用actor的前向传播)
            # tour_indices, tour_logp = actor(static, dynamic, init_depot, x0)
            tour_indices, tour_logp = actor(static, dynamic, x0)

            # tour_indices=torch.cat((init_depot,tour_indices),dim=1)
            # print("Train：tour indice:",tour_indices)

            # Sum the log probabilities for each city in the tour(每个城市的对数几率和，作为真实奖励值)
            reward = reward_fn(static, tour_indices, depot_num)

            # Query the critic for an estimate of the reward(向评论家询问奖励的估计值)
            critic_est = critic(static, dynamic).view(-1)
            # 真实奖励值和估计奖励值的差，作为优势函数(这里是A2C中的advantage)
            advantage = (reward - critic_est)
            # actor_loss是优势函数乘以演员的动作概率分布，这个乘积表示每个动作的优势加权的动作概率。然后取平均值作为演员的损失
            actor_loss = torch.mean(advantage.detach() * tour_logp.sum(dim=1))
            # critic_loss是根据优势函数的平方误差计算的
            critic_loss = torch.mean(advantage ** 2)
            # 0梯度反向传播
            actor_optim.zero_grad()
            actor_loss.backward()
            # 对模型的梯度进行裁剪，以防止梯度爆炸问题
            torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)
            actor_optim.step()

            critic_optim.zero_grad()
            critic_loss.backward()
            torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)
            critic_optim.step()
            # 将奖励估计值，真实奖励值，actor损失平均求和后写入空列表中
            critic_rewards.append(torch.mean(critic_est.detach()).item())
            rewards.append(torch.mean(reward.detach()).item())
            losses.append(torch.mean(actor_loss.detach()).item())  # loss放每个batch的损失

            # 每100次输出
            if (batch_idx + 1) % 100 == 0:
                end = time.time()
                times.append(end - start)
                start = end

                mean_loss = np.mean(losses[-100:])
                mean_reward = np.mean(rewards[-100:])

                print('  Batch %d/%d, reward: %2.3f, loss: %2.4f, took: %2.4fs' %
                      (batch_idx, len(train_loader), mean_reward, mean_loss,
                       times[-1]))
        ###暂时绘图loss和reward。【当前epoch的每个
        averages_loss = [np.mean(np.array(losses)[i:i + 100]) for i in range(0, len(losses), 100)]
        averages_reward = [np.mean(np.array(rewards)[i:i + 100]) for i in range(0, len(rewards), 100)]

        x = np.arange(len(averages_loss))
        plt.figure(1)
        plt.plot(x, averages_loss)
        plt.title('averages_loss')
        plt.grid(True)
        plt.figure(2)
        plt.plot(x, averages_reward)
        plt.title('averages_reward')
        plt.grid(True)
        #############
        # 绘制所有epoch的loss和reward
        all_epoch_loss.extend(averages_loss)
        all_epoch_reward.extend(averages_reward)
        # plt.figure(3)
        # plt.plot(np.arange(len(all_epoch_loss)), all_epoch_loss)
        # plt.title('all_epoch_loss')
        # plt.grid(True)
        # plt.figure(4)
        # plt.plot(np.arange(len(all_epoch_reward)), all_epoch_reward)
        # plt.title('all_epoch_reward')
        # plt.grid(True)
        # plt.show()
        #############
        mean_loss = np.mean(losses)
        mean_reward = np.mean(rewards)

        # Save the weights
        epoch_dir = os.path.join(checkpoint_dir, '%s' % epoch)
        if not os.path.exists(epoch_dir):
            os.makedirs(epoch_dir)

        save_path = os.path.join(epoch_dir, 'actor.pt')
        torch.save(actor.state_dict(), save_path)

        save_path = os.path.join(epoch_dir, 'critic.pt')
        torch.save(critic.state_dict(), save_path)

        # Save rendering of validation set tours(把验证集数据放入validation中，主要获得索引并且绘图)
        valid_dir = os.path.join(save_dir, '%s' % epoch)

        mean_valid = validate(valid_loader, actor, reward_fn, render_fn,
                              valid_dir, num_plot=5, depot_number=depot_num)

        # Save best model parameters(保存最佳奖励)
        if mean_valid < best_reward:
            best_reward = mean_valid

            save_path = os.path.join(save_dir, 'actor.pt')
            torch.save(actor.state_dict(), save_path)

            save_path = os.path.join(save_dir, 'critic.pt')
            torch.save(critic.state_dict(), save_path)
        # 输出平均(actor)损失，平均奖励，平均验证奖励，运行时间
        print('Mean epoch loss/reward: %2.4f, %2.4f, %2.4f, took: %2.4fs ' \
              '(%2.4fs / 100 batches)\n' % \
              (mean_loss, mean_reward, mean_valid, time.time() - epoch_start,
               np.mean(times)))
    plt.figure(3)
    plt.plot(np.arange(len(all_epoch_loss)), all_epoch_loss)
    plt.title('all_epoch_loss')
    plt.grid(True)
    plt.figure(4)
    plt.plot(np.arange(len(all_epoch_reward)), all_epoch_reward)
    plt.title('all_epoch_reward')
    plt.grid(True)
    plt.show()


def train_vrp(args):
    # Goals from paper: 【注意这是纯vrp问题，没有加上路径损耗。可不能直接比……】
    # VRP10, Capacity 20:  4.84  (Greedy)
    # VRP20, Capacity 30:  6.59  (Greedy)
    # VRP50, Capacity 40:  11.39 (Greedy)
    # VRP100, Capacity 50: 17.23  (Greedy)

    # from tasks import vrp # 测试注释掉
    # import vrp
    # from tasks.vrp import VehicleRoutingDataset

    # Determines the maximum amount of load for a vehicle based on num nodes
    LOAD_DICT = {10: 20, 20: 30, 30: 35, 50: 40, 100: 50, 200: 80}  # todo 以后改
    MAX_DEMAND = 1
    STATIC_SIZE = 2  # (x, y)
    DYNAMIC_SIZE = 2  # (load, demand)

    max_load = LOAD_DICT[args.num_nodes]
    car_load = 30.  ######################### fixme 之后修改吧先不动了。
    map_size = 1  # fixme 似乎地图大小固定为1……这样改真的不会出问题吗
    # 生成随机训练数据集(1000000)，验证数据集(1000)
    train_data = VehicleRoutingDataset(args.train_size,
                                       args.num_nodes,
                                       max_load,
                                       car_load,
                                       MAX_DEMAND,
                                       args.seed,
                                       args.depot_num)

    valid_data = VehicleRoutingDataset(args.valid_size,
                                       args.num_nodes,
                                       max_load,
                                       car_load,
                                       MAX_DEMAND,
                                       args.seed + 1,
                                       args.depot_num)
    # print(type(train_data))
    # 实例化actor
    actor = DRL4TSP(STATIC_SIZE,
                    DYNAMIC_SIZE,
                    args.hidden_size,
                    car_load,
                    train_data.update_dynamic,
                    train_data.update_mask2,
                    train_data.node_distance2,
                    args.num_layers,
                    args.dropout).to(device)
    # 实例化critic
    critic = StateCritic(STATIC_SIZE, DYNAMIC_SIZE, args.hidden_size).to(device)

    # 转换为字典，并且添加训练数据，验证数据，reward奖励函数，render渲染函数
    kwargs = vars(args)
    kwargs['train_data'] = train_data
    kwargs['valid_data'] = valid_data
    # kwargs['reward_fn'] = vrp.reward # colab上不用vrp句柄
    # kwargs['render_fn'] = vrp.render
    kwargs['reward_fn'] = reward
    kwargs['render_fn'] = render

    if args.checkpoint:  # 保存模型
        print("args.checkpoint:已经有ckpt，开始读取。")
        path = os.path.join(args.checkpoint, 'actor.pt')
        actor.load_state_dict(torch.load(path, device))  # load_state_dict：加载模型参数

        path = os.path.join(args.checkpoint, 'critic.pt')
        critic.load_state_dict(torch.load(path, device))  # 加载模型参数

    if not args.test:
        print("not args.test：模型开始训练")
        train(actor, critic, **kwargs)  # 训练！!!!!!!!!!!!!!!!!!!!!

    # 生成测试数据，大小于验证数据一致(1000)
    test_data = VehicleRoutingDataset(args.valid_size,
                                      args.num_nodes,
                                      max_load,
                                      car_load,
                                      MAX_DEMAND,
                                      args.seed + 2,
                                      args.depot_num)

    test_dir = 'test_picture'
    test_loader = DataLoader(test_data, args.batch_size, False, num_workers=0)
    # out = validate(test_loader, actor, vrp.reward, vrp.render, test_dir, num_plot=5)
    out = validate(test_loader, actor, reward, render, test_dir, num_plot=5, depot_number=args.depot_num)

    print('Average tour length: ', out)


if __name__ == '__main__':
    # 建了一个命令行参数解析器对象 parser
    parser = argparse.ArgumentParser(description='Combinatorial Optimization')
    parser.add_argument('--seed', default=1234, type=int)
    parser.add_argument('--checkpoint', default=None)
    parser.add_argument('--test', action='store_true', default=False)
    parser.add_argument('--task', default='vrp')
    parser.add_argument('--nodes', dest='num_nodes', default=50, type=int)  # city数量##############
    # parser.add_argument('--actor_lr', default=5e-4, type=float)
    # parser.add_argument('--critic_lr', default=5e-4, type=float)
    parser.add_argument('--actor_lr', default=1e-4, type=float)  # 学习率，现在在训练第4epoch，我手动改了一下。……………………
    parser.add_argument('--critic_lr', default=1e-4, type=float)
    parser.add_argument('--max_grad_norm', default=2., type=float)
    # parser.add_argument('--batch_size', default=64, type=int)  ##########################
    parser.add_argument('--batch_size', default=8, type=int)
    parser.add_argument('--hidden', dest='hidden_size', default=128, type=int)
    parser.add_argument('--dropout', default=0.1, type=float)
    parser.add_argument('--layers', dest='num_layers', default=1, type=int)
    # parser.add_argument('--train-size',default=1000000, type=int)#fixme!!!!!!!!!!!!
    parser.add_argument('--train-size', default=10, type=int)  # fixme!!!!!!!!!!!!####################
    parser.add_argument('--valid-size', default=100, type=int)
    parser.add_argument('--depot_num', default=5, type=int)  ###############
    # 解析为args
    # args = parser.parse_args() #若在本地跑请使用本行代码
    args = parser.parse_known_args()[0]  # colab环境跑使用

    # --------------------------------------------------------------------
    args.test = True
    # args.test = False
    # --------------------------------------------------------------------

    # print('NOTE: SETTTING CHECKPOINT: ')
    # args.checkpoint = os.path.join('vrp', '10', '12_59_47.350165' + os.path.sep)
    # print(args.checkpoint)

    # # 设置checkpoint路径######################################
    # args.checkpoint = '/content/drive/MyDrive/vrp/100/12_57_12.349803/checkpoints/1' # 这是黎的
    args.checkpoint = "test4"  # 这是lw的google模型文件夹名
    args.checkpoint = "trained_model"  # 这是lw的本地文件夹
    # args.checkpoint ="/content/drive/MyDrive/"

    if args.task == 'vrp':
        train_vrp(args)
    else:
        raise ValueError('Task <%s> not understood' % args.task)
